{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 2.1 Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "2.1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    #pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.sum(self.weights*inputs) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = 3.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.8400)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Neuron(weights, bias)\n",
    "\n",
    "res = neuron.forward(inputs)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37"
   },
   "source": [
    "2.1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "fVWF3a9vtx90"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, weights, biases):\n",
    "    self.weights = weights\n",
    "    self.biases = biases\n",
    "    #pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.matmul(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Fo-JFnHPuFCS"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4.8400,  0.1700, 10.3900])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(weights, biases)\n",
    "res = linear.forward(inputs)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd"
   },
   "source": [
    "2.1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Z8IizmtsuhO1"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.7900,  0.9200,  9.0850],\n        [ 6.1400, -2.1000,  6.9000],\n        [ 2.0400,  0.7610,  6.7260]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = linear.forward(inputs)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "2.1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "IOv52EdovASs"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    self.n_features = n_features\n",
    "    self.n_neurons = n_neurons\n",
    "    self.weights = torch.randn(self.n_features, self.n_neurons)\n",
    "    self.biases = torch.randn(self.n_neurons)\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.matmul(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.8101,  8.6537,  4.0937],\n        [ 1.4962, -1.2842,  3.4435],\n        [ 1.8217,  7.4116,  2.2414]])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(4, 3)\n",
    "linear.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "2.1.5 Используя решение из __2.1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "RjjQIQlTxJE6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  9.9509,   7.7411,   4.4733,   5.0301,   3.3596,  14.2657,   5.2831],\n        [ 16.8953,   1.7149, -13.4990,   0.0608,  -3.7141,  26.3046,  12.4055],\n        [  4.2300,  10.5011,  20.2487,   6.8382,   8.2724,   3.5464,   8.7085]])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "linear_1 = Linear(4, 3)\n",
    "linear_2 = Linear(3, 7)\n",
    "\n",
    "linear_2.forward(linear_1.forward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "  ## 2.2 Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def forward(self, inputs):\n",
    "    mask = inputs < 0\n",
    "    inputs[mask] = 0\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.4852, 0.0000],\n",
      "        [1.2609, 0.0000, 0.0000],\n",
      "        [1.1314, 1.5944, 0.0745],\n",
      "        [0.0000, 0.0000, 1.9929]])\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "matr = torch.randn((4, 3))\n",
    "res = relu.forward(matr)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [],
   "source": [
    "class Softmax_c:\n",
    "  def forward(self, inputs):\n",
    "    matr = torch.exp(inputs)\n",
    "    return matr/torch.sum(inputs, dim=1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.4668,  -0.0446,  -0.4710],\n",
      "        [ -1.0709,  -2.0452, -15.2172],\n",
      "        [  0.3803,   0.3954,   5.0436],\n",
      "        [  2.4555,   0.4762,   2.0623]])\n"
     ]
    }
   ],
   "source": [
    "sf = Softmax_c()\n",
    "matr = torch.randn((4, 3))\n",
    "res = sf.forward(matr)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [],
   "source": [
    "class ELU:\n",
    "  def __init__(self, alpha):\n",
    "    self.alpha = alpha\n",
    "    pass\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    inputs[inputs < 0] = (torch.exp(inputs[inputs < 0]) - 1) * self.alpha\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[-0.4468, -2.8943,  0.1120],\n",
      "        [-0.0720, -0.1327, -1.1924],\n",
      "        [-1.3818,  0.2383,  0.8949],\n",
      "        [ 0.7810,  1.2252,  0.5714]])\n",
      "Output:\n",
      " tensor([[-0.3604, -0.9447,  0.1120],\n",
      "        [-0.0694, -0.1243, -0.6965],\n",
      "        [-0.7489,  0.2383,  0.8949],\n",
      "        [ 0.7810,  1.2252,  0.5714]])\n"
     ]
    }
   ],
   "source": [
    "elu = ELU(alpha = 1)\n",
    "matr = torch.randn((4, 3))\n",
    "print(\"Input:\\n\", matr)\n",
    "\n",
    "res = elu.forward(matr)\n",
    "print(\"Output:\\n\", res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 2.3 Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "2.3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3595)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(4, 1)\n",
    "\n",
    "mse_loss = MSELoss()\n",
    "res = mse_loss.forward(linear_layer.forward(inputs), y)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "2.3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossentropyLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return -1 * (y_true * torch.log(y_pred)).sum(1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.9740, -7.0203, -3.4153])\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(4, 3) #Слой с 3 нейронами\n",
    "\n",
    "softmax = Softmax_c()\n",
    "cce_loss = CategoricalCrossentropyLoss()\n",
    "res = cce_loss.forward(softmax.forward(linear_layer.forward(inputs)), y)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6dbanf44_4"
   },
   "source": [
    "2.3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ADsZxD-h4_Os"
   },
   "outputs": [],
   "source": [
    "class MSELossL2:\n",
    "    def __init__(self, lambda_, layer, W=0.1):\n",
    "        self.lambda_ = lambda_\n",
    "        self.layer = layer\n",
    "        self.layer.W = W\n",
    "\n",
    "    def data_loss(self, y_pred, y_true):\n",
    "        return (y_true - y_pred) ** 2\n",
    "\n",
    "    def reg_loss(self, layer):\n",
    "        return self.lambda_ * (layer.W ** 2)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return self.data_loss(y_pred, y_true) + self.reg_loss(self.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.9316e-01, 1.7339e-02, 1.1416e+02],\n",
      "        [9.7086e-01, 1.9341e+03, 2.3317e+00],\n",
      "        [2.8711e+04, 1.5001e-02, 2.4423e+06]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                        [2, 5, -1, 2],\n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])\n",
    "\n",
    "linear_layer = Linear(4, 3)\n",
    "layer_out = linear_layer.forward(inputs)\n",
    "\n",
    "softmax = Softmax_c()\n",
    "mse_loss = MSELossL2(1.5, linear_layer)\n",
    "res = mse_loss.forward(softmax.forward(layer_out), y)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w049ZSdR6qQi"
   },
   "source": [
    "## 2.4 Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtCfSME9W7Q"
   },
   "source": [
    "2.4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "4xmI-QJ66WAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.5097, -0.4381, -1.2528,  0.7775],\n        [-0.6743,  0.0318, -0.6358,  0.6764],\n        [-0.8878, -1.9808, -0.3479,  0.1563],\n        [ 1.9559,  0.3901, -0.6524, -0.3910],\n        [ 0.0977,  0.5830, -0.3994,  0.3701],\n        [-1.8431, -0.4780, -0.4797,  0.6204],\n        [ 0.8416, -0.2495,  0.0495,  0.4938],\n        [-0.7196, -0.8130,  0.2745, -0.8909],\n        [-0.0157,  0.1609, -0.1907, -0.3948],\n        [-0.3090, -1.6760,  1.1523,  1.0796],\n        [-0.7256, -1.3834, -1.5829,  0.6104],\n        [ 2.3039, -1.0600, -0.1359,  1.1369],\n        [ 0.5766, -0.2083,  0.3960, -1.0931],\n        [ 1.4941, -0.2052,  0.3131, -0.8541],\n        [-0.3534, -1.6165, -0.2918, -0.7615],\n        [ 0.0665,  0.3025, -0.6343, -0.3627],\n        [-0.2677, -1.1280,  0.2804, -0.9931],\n        [-0.5973, -0.2379, -1.4241, -0.4933],\n        [ 0.6433, -1.5706, -0.2069,  0.8802],\n        [ 1.8832, -1.3478, -1.2705,  0.9694],\n        [-0.9073,  0.0519,  0.7291,  0.1290],\n        [-0.1105,  1.0202, -0.6920,  1.5364],\n        [-1.1574, -0.3123, -0.1577,  2.2567],\n        [ 0.0105,  1.7859,  0.1269,  0.4020],\n        [-1.1651,  0.9008,  0.4657, -1.5362],\n        [-0.4409, -0.2804, -0.3647,  0.1567],\n        [ 1.9229,  1.4805,  1.8676,  0.9060],\n        [-0.8034, -0.6895, -0.4555,  0.0175],\n        [-1.4913,  0.4394,  0.1667,  0.6350],\n        [ 0.7610,  0.1217,  0.4439,  0.3337],\n        [ 1.1880,  0.3169,  0.9209,  0.3187],\n        [-1.5408,  0.0633,  0.1565,  0.2322],\n        [-1.9363,  0.1888,  0.5239,  0.0884],\n        [ 0.2083,  0.9766,  0.3564,  0.7066],\n        [ 0.7733, -1.1839, -2.6592,  0.6063],\n        [ 0.2799, -0.0982,  0.9102,  0.3172],\n        [-0.5429,  0.4161, -1.1562,  0.7812],\n        [ 1.5328,  1.4694,  0.1549,  0.3782],\n        [-1.2254,  0.8444, -1.0002, -1.5448],\n        [-0.3540, -1.3750, -0.6436, -2.2234],\n        [ 0.6985,  0.0038,  0.9318,  0.3400],\n        [-0.9559, -0.3460, -0.4636,  0.4815],\n        [-0.8612,  1.9101, -0.2680,  0.8025],\n        [ 0.6938, -0.1596, -0.1337,  1.0777],\n        [ 0.6252, -1.6021, -1.1044,  0.0522],\n        [-0.7047,  0.9433,  0.7472, -1.1889],\n        [ 2.3831,  0.9445, -0.9128,  1.1170],\n        [-0.3109,  0.0974,  0.3990, -2.7726],\n        [-0.8708, -0.5788, -0.3116,  0.0562],\n        [-1.0708,  1.0545, -0.4032,  1.2224],\n        [ 1.1266, -1.0799, -1.1475, -0.4378],\n        [ 0.4937, -0.1161, -2.0307,  2.0645],\n        [-0.0170,  0.3792,  2.2593, -0.0423],\n        [ 1.7641,  0.4002,  0.9787,  2.2409],\n        [ 0.5785,  0.3497, -0.7641, -1.4378],\n        [-1.6139, -0.2127, -0.8955,  0.3869],\n        [ 1.6481,  0.1642,  0.5673, -0.2227],\n        [-0.6374, -0.3973, -0.1329, -0.2978],\n        [ 0.1774, -0.4018, -1.6302,  0.4628],\n        [ 0.2863,  0.6088, -1.0453,  1.2111],\n        [ 0.6723,  0.4075, -0.7699,  0.5392],\n        [-0.6946, -0.1496, -0.4352,  1.8493],\n        [ 0.7718,  0.8235,  2.1632,  1.3365],\n        [ 1.4945, -2.0700,  0.4263,  0.6769],\n        [-1.3159, -0.4616, -0.0682,  1.7133],\n        [ 0.1420, -0.3193,  0.6915,  0.6947],\n        [-0.6725, -0.3596, -0.8131, -1.7263],\n        [-1.7559,  0.4509, -0.6840,  1.6596],\n        [ 0.3764, -1.0994,  0.2982,  1.3264],\n        [-1.1731,  1.9436, -0.4136, -0.7475],\n        [-0.4980,  1.9295,  0.9494,  0.0876],\n        [-0.0393, -1.1681,  0.5233, -0.1715],\n        [-1.6981,  0.3873, -2.2556, -1.0225],\n        [-0.5108, -1.1806, -0.0282,  0.4283],\n        [ 1.1394, -1.2348,  0.4023, -0.6848],\n        [ 1.4883,  1.8959,  1.1788, -0.1799],\n        [-1.1268, -0.7307, -0.3849,  0.0944],\n        [-0.7396,  1.5430, -1.2929,  0.2671],\n        [ 1.3645, -0.6894, -0.6523, -0.5212],\n        [-0.8134, -1.4664,  0.5211, -0.5758],\n        [-0.0422, -0.2869, -0.0616, -0.1073],\n        [ 0.8568, -0.6510, -1.0342,  0.6816],\n        [-1.1889, -0.5068, -0.5963, -0.0526],\n        [ 0.7863, -0.4664, -0.9444, -0.4100],\n        [-2.5530,  0.6536,  0.8644, -0.7422],\n        [ 0.6898,  1.3018, -0.6281, -0.4810],\n        [ 1.2303,  1.2024, -0.3873, -0.3023],\n        [ 0.9473, -0.1550,  0.6141,  0.9222],\n        [ 0.6664, -0.4607, -1.3343, -1.3467],\n        [-0.1032,  0.4106,  0.1440,  1.4543],\n        [ 0.6401, -1.6170, -0.0243, -0.7380],\n        [-0.3692, -0.2394,  1.0997,  0.6553],\n        [ 1.0685, -0.4534, -0.6878, -1.2141],\n        [-1.3065,  1.6581, -0.1182, -0.6802],\n        [ 0.8579,  1.1411,  1.4666,  0.8526],\n        [ 0.0386, -1.6567, -0.9855, -1.4718],\n        [ 2.2698, -1.4544,  0.0458, -0.1872],\n        [-1.0486, -1.4200, -1.7063,  1.9508],\n        [ 1.8676, -0.9773,  0.9501, -0.1514],\n        [-0.7448, -0.8264, -0.0985, -0.6635]])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5, random_state=0)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpPSPYSpD9Ey"
   },
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc1sXtGd_J-y"
   },
   "source": [
    "2.4.1.1 Реализуйте класс `SquaredLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "llFigkqd_JRU"
   },
   "outputs": [],
   "source": [
    "class SquaredLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return (y_pred - y_true) ** 2\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = 2 * (y_pred - y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7ForfM97UQ"
   },
   "source": [
    "2.4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs: int):\n",
    "    self.n_inputs = n_inputs\n",
    "    self.weights = torch.randn(n_inputs)\n",
    "    self.bias = torch.randn(1)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    return (self.inputs * self.weights).sum() + self.bias\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
    "    # df/dW\n",
    "    self.dweights = dvalue * self.inputs\n",
    "    # df/wX\n",
    "    self.dinput =  dvalue * self.weights\n",
    "    # df/db\n",
    "    self.dbias = dvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4518])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "naive_neuron = Neuron(4)\n",
    "out = naive_neuron.forward(inputs)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "2.4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": "[tensor([2138.7148]),\n tensor([358.7410]),\n tensor([1.4164]),\n tensor([1.2858e-07]),\n tensor([3.5225e-06]),\n tensor([2.7472e-05]),\n tensor([1.3272e-06]),\n tensor([6.7288e-08]),\n tensor([0.]),\n tensor([3.6380e-12]),\n tensor([5.8208e-11]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([0.]),\n tensor([1.3097e-10]),\n tensor([1.4552e-11]),\n tensor([5.8208e-11]),\n tensor([0.]),\n tensor([0.]),\n tensor([7.1304e-10]),\n tensor([0.]),\n tensor([2.3283e-10]),\n tensor([1.4552e-11]),\n tensor([3.2742e-11]),\n tensor([2.3283e-10]),\n tensor([0.]),\n tensor([2.3283e-10]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12])]"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X.shape[1] # <размерность элемента выборки >\n",
    "print(n_inputs)\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = SquaredLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "  for x_example, y_example in zip(X, y):\n",
    "    # нейрон\n",
    "    y_pred =  neuron.forward(x_example)\n",
    "    # функция потерь\n",
    "    curr_loss = loss.forward(y_pred, y_example)\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "    #backward\n",
    "    loss.backward(y_pred, y_example)\n",
    "    neuron.backward(loss.dinput)\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "    neuron.weights -= learning_rate * neuron.dweights\n",
    "    neuron.bias -= learning_rate * neuron.dbias\n",
    "losses[::20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "2.4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "2.4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = 2 * (y_pred - y_true) / y_pred.shape[0] # df/dy^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "2.4.2.2. Модифицируйте класс `Neuron` из __2.4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "o_OpuAP0Jpz1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    self.n_inputs = n_inputs\n",
    "    self.weights = torch.randn(1, n_inputs).T\n",
    "    self.bias = torch.randn(1)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    return torch.matmul(inputs, self.weights) + self.bias\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
    "    self.dinputs = torch.matmul(dvalue, self.weights.T)\n",
    "    self.dweights = torch.matmul(self.inputs.T, dvalue) # df/dW\n",
    "    self.dbias = torch.sum(dvalue) # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "2.4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Zqwm_7eqJim1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(14383.0850),\n tensor(6161.0527),\n tensor(2693.2324),\n tensor(1204.8553),\n tensor(553.1680),\n tensor(261.2742),\n tensor(127.1657),\n tensor(63.8119),\n tensor(32.9872),\n tensor(17.5316),\n tensor(9.5510),\n tensor(5.3154),\n tensor(3.0112),\n tensor(1.7307),\n tensor(1.0062),\n tensor(0.5904),\n tensor(0.3488),\n tensor(0.2073),\n tensor(0.1237),\n tensor(0.0740),\n tensor(0.0444),\n tensor(0.0267),\n tensor(0.0161),\n tensor(0.0097),\n tensor(0.0059),\n tensor(0.0035),\n tensor(0.0021),\n tensor(0.0013),\n tensor(0.0008),\n tensor(0.0005),\n tensor(0.0003),\n tensor(0.0002),\n tensor(0.0001),\n tensor(6.4311e-05),\n tensor(3.9064e-05),\n tensor(2.3756e-05),\n tensor(1.4415e-05),\n tensor(8.7592e-06),\n tensor(5.3504e-06),\n tensor(3.2811e-06),\n tensor(2.0157e-06),\n tensor(1.2647e-06),\n tensor(7.8168e-07),\n tensor(5.2781e-07),\n tensor(3.5335e-07),\n tensor(2.1805e-07),\n tensor(1.3391e-07),\n tensor(1.1484e-07),\n tensor(1.1479e-07),\n tensor(1.1185e-07)]"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X.shape[1] # размерность\n",
    "learning_rate = 0.01 #  скорость\n",
    "n_epoch = 1000 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # forward pass\n",
    "    y_pred = neuron.forward(X) # нейрон\n",
    "    curr_loss = loss.forward(y_pred, y) #функция потерь\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "\n",
    "    #backward\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "    loss.backward(y_pred, y)\n",
    "    neuron.backward(loss.dinput)\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "    neuron.weights -= learning_rate * neuron.dweights\n",
    "    neuron.bias -= learning_rate * neuron.dbias\n",
    "\n",
    "losses[::20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "2.4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "2.4.3.1 Модифицируйте класс `Linear` из __2.1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "9zWuhaLdSB2_"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3719429702.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [92]\u001B[1;36m\u001B[0m\n\u001B[1;33m    self.dweights = # df/dW\u001B[0m\n\u001B[1;37m                    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику слоя>\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dweights = # df/dW\n",
    "    self.dbiases = # df/db\n",
    "    self.dinputs = # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "2.4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "2.4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = inputs.clip(min=0)\n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.clone()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "  # <forward pass>\n",
    "  # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "  data_loss = # <прогон через функцию потерь>\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(f'epoch {epoch} mean loss {data_loss}')\n",
    "    ys.append(out)\n",
    "  \n",
    "  # <backprop> \n",
    "  # loss > fc2 > relu1 > fc1\n",
    "\n",
    "  # <шаг оптимизации для fc1>\n",
    "\n",
    "  # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
    "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "  ax.set_xlim(-1.05, 1.5)\n",
    "  ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
