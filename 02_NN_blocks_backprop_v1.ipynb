{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 2.1 Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "2.1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    #pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.sum(self.weights*inputs) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = 3.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.8400)"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Neuron(weights, bias)\n",
    "\n",
    "res = neuron.forward(inputs)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37"
   },
   "source": [
    "2.1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "fVWF3a9vtx90"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, weights, biases):\n",
    "    self.weights = weights\n",
    "    self.biases = biases\n",
    "    #pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.matmul(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "Fo-JFnHPuFCS"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4.8400,  0.1700, 10.3900])"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(weights, biases)\n",
    "res = linear.forward(inputs)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd"
   },
   "source": [
    "2.1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "Z8IizmtsuhO1"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.7900,  0.9200,  9.0850],\n        [ 6.1400, -2.1000,  6.9000],\n        [ 2.0400,  0.7610,  6.7260]])"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = linear.forward(inputs)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "2.1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "IOv52EdovASs"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    self.n_features = n_features\n",
    "    self.n_neurons = n_neurons\n",
    "    self.weights = torch.randn(self.n_features, self.n_neurons)\n",
    "    self.biases = torch.randn(self.n_neurons)\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.matmul(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.3190,  3.0288, -1.5835],\n        [-2.3141, -3.2246, -4.7458],\n        [-2.0571, -3.9855,  1.4061]])"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(4, 3)\n",
    "linear.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "2.1.5 Используя решение из __2.1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "RjjQIQlTxJE6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  2.8364,   1.7682,  -1.2549,  -2.3819,  -1.4404,   1.2397,   1.8018],\n        [  0.2004,   3.2024,  -7.2196, -14.9859,  -6.9387,   9.1500,  13.4043],\n        [ -3.5855,   5.7864,   2.5832,  -4.6882,  -9.0695,  10.8442,  -2.7183]])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "linear_1 = Linear(4, 3)\n",
    "linear_2 = Linear(3, 7)\n",
    "\n",
    "linear_2.forward(linear_1.forward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "  ## 2.2 Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def forward(self, inputs):\n",
    "    mask = inputs < 0\n",
    "    inputs[mask] = 0\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 1.4244],\n",
      "        [0.0000, 0.5588, 0.0000],\n",
      "        [0.0580, 0.0000, 0.0000],\n",
      "        [1.3891, 0.7534, 0.3275]])\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "matr = torch.randn((4, 3))\n",
    "res = relu.forward(matr)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [],
   "source": [
    "class Softmax_c:\n",
    "  def forward(self, inputs):\n",
    "    matr = torch.exp(inputs)\n",
    "    return matr/torch.sum(inputs, dim=1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4414,  0.2188,  4.0548],\n",
      "        [ 1.8485, 17.3876,  0.3991],\n",
      "        [ 3.8198,  1.5467,  2.4205],\n",
      "        [76.7262, 35.7023, 13.0922]])\n"
     ]
    }
   ],
   "source": [
    "sf = Softmax_c()\n",
    "matr = torch.randn((4, 3))\n",
    "res = sf.forward(matr)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [],
   "source": [
    "class ELU:\n",
    "  def __init__(self, alpha):\n",
    "    self.alpha = alpha\n",
    "    pass\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    inputs[inputs < 0] = (torch.exp(inputs[inputs < 0]) - 1) * self.alpha\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[ 0.8724,  1.7565,  0.7859],\n",
      "        [-0.1431,  0.8570,  0.0966],\n",
      "        [-1.8525,  0.5196,  0.2690],\n",
      "        [-0.5411, -0.2260, -0.1513]])\n",
      "Output:\n",
      " tensor([[ 0.8724,  1.7565,  0.7859],\n",
      "        [-0.1333,  0.8570,  0.0966],\n",
      "        [-0.8432,  0.5196,  0.2690],\n",
      "        [-0.4179, -0.2023, -0.1404]])\n"
     ]
    }
   ],
   "source": [
    "elu = ELU(alpha = 1)\n",
    "matr = torch.randn((4, 3))\n",
    "print(\"Input:\\n\", matr)\n",
    "\n",
    "res = elu.forward(matr)\n",
    "print(\"Output:\\n\", res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 2.3 Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "2.3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.8290)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(4, 1)\n",
    "\n",
    "mse_loss = MSELoss()\n",
    "res = mse_loss.forward(linear_layer.forward(inputs), y)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "2.3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossentropyLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return -1 * (y_true * torch.log(y_pred)).sum(1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    nan, -0.4036,     nan])\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(4, 3) #Слой с 3 нейронами\n",
    "\n",
    "softmax = Softmax_c()\n",
    "cce_loss = CategoricalCrossentropyLoss()\n",
    "res = cce_loss.forward(softmax.forward(linear_layer.forward(inputs)), y)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6dbanf44_4"
   },
   "source": [
    "2.3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "ADsZxD-h4_Os"
   },
   "outputs": [],
   "source": [
    "class MSELossL2:\n",
    "    def __init__(self, lambda_, layer, W=0.1):\n",
    "        self.lambda_ = lambda_\n",
    "        self.layer = layer\n",
    "        self.layer.W = W\n",
    "\n",
    "    def data_loss(self, y_pred, y_true):\n",
    "        return (y_true - y_pred) ** 2\n",
    "\n",
    "    def reg_loss(self, layer):\n",
    "        return self.lambda_ * (layer.W ** 2)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return self.data_loss(y_pred, y_true) + self.reg_loss(self.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4438e+02, 1.5034e-02, 1.6712e+02],\n",
      "        [2.8172e+04, 1.5004e-02, 4.6064e+00],\n",
      "        [1.6329e-02, 3.1593e+04, 1.5071e-02]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                        [2, 5, -1, 2],\n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])\n",
    "\n",
    "linear_layer = Linear(4, 3)\n",
    "layer_out = linear_layer.forward(inputs)\n",
    "\n",
    "softmax = Softmax_c()\n",
    "mse_loss = MSELossL2(1.5, linear_layer)\n",
    "res = mse_loss.forward(softmax.forward(layer_out), y)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w049ZSdR6qQi"
   },
   "source": [
    "## 2.4 Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtCfSME9W7Q"
   },
   "source": [
    "2.4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "import sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "4xmI-QJ66WAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -41.2726],\n        [   4.2447],\n        [ -95.1568],\n        [ -24.5819],\n        [  27.9462],\n        [ -31.4934],\n        [  55.9900],\n        [-101.7946],\n        [ -41.9318],\n        [ 109.7482],\n        [-115.0168],\n        [ 102.2581],\n        [ -64.1279],\n        [ -29.8171],\n        [-148.6656],\n        [ -62.5905],\n        [-111.8847],\n        [-159.5456],\n        [  23.4118],\n        [  -7.6453],\n        [  44.3271],\n        [ 121.3781],\n        [ 153.8689],\n        [ 105.6647],\n        [ -96.1774],\n        [ -28.9994],\n        [ 296.4494],\n        [ -68.7938],\n        [  52.0599],\n        [  79.6068],\n        [ 125.9714],\n        [   2.0849],\n        [  10.4737],\n        [ 124.3630],\n        [-150.6220],\n        [  92.3236],\n        [  -5.9085],\n        [ 125.8456],\n        [-199.2208],\n        [-292.7495],\n        [ 107.8483],\n        [ -19.9285],\n        [ 100.5492],\n        [  94.9820],\n        [-111.5261],\n        [ -35.7181],\n        [ 118.0920],\n        [-219.3334],\n        [ -53.2541],\n        [  94.8052],\n        [-129.4050],\n        [  50.8446],\n        [ 162.1754],\n        [ 313.5364],\n        [-153.7874],\n        [ -66.3793],\n        [  58.6700],\n        [ -61.3061],\n        [ -79.1444],\n        [  62.9756],\n        [  23.5475],\n        [ 114.3204],\n        [ 308.2542],\n        [  48.7361],\n        [ 103.7899],\n        [ 100.3471],\n        [-232.3353],\n        [  79.5837],\n        [ 107.4361],\n        [ -50.8164],\n        [ 128.1276],\n        [ -19.9153],\n        [-263.4981],\n        [ -14.5550],\n        [ -51.3476],\n        [ 159.6745],\n        [ -65.2899],\n        [ -25.8793],\n        [ -85.0315],\n        [ -81.6639],\n        [ -23.7692],\n        [ -14.1987],\n        [ -86.1275],\n        [ -99.2442],\n        [ -36.2797],\n        [ -25.6477],\n        [  14.0245],\n        [ 137.2250],\n        [-210.2233],\n        [ 150.0205],\n        [-108.1685],\n        [ 116.7318],\n        [-146.3562],\n        [ -37.4102],\n        [ 231.2077],\n        [-251.3713],\n        [ -16.0462],\n        [ -13.3754],\n        [  56.3185],\n        [-107.9941]])"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5, random_state=0)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpPSPYSpD9Ey"
   },
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc1sXtGd_J-y"
   },
   "source": [
    "2.4.1.1 Реализуйте класс `SquaredLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "llFigkqd_JRU"
   },
   "outputs": [],
   "source": [
    "class SquaredLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return (y_pred - y_true) ** 2\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = 2 * (y_pred - y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7ForfM97UQ"
   },
   "source": [
    "2.4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs: int):\n",
    "    self.n_inputs = n_inputs\n",
    "    self.weights = torch.randn(n_inputs)\n",
    "    self.bias = torch.randn(1)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    return (self.inputs * self.weights).sum() + self.bias\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
    "    # df/dW\n",
    "    self.dweights = dvalue * self.inputs\n",
    "    # df/wX\n",
    "    self.dinput =  dvalue * self.weights\n",
    "    # df/db\n",
    "    self.dbias = dvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.4917])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "naive_neuron = Neuron(4)\n",
    "out = naive_neuron.forward(inputs)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "2.4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": "[tensor([1466.1143]),\n tensor([342.5593]),\n tensor([1.2629]),\n tensor([1.3692e-07]),\n tensor([3.6307e-06]),\n tensor([2.4178e-05]),\n tensor([1.1903e-06]),\n tensor([5.9605e-08]),\n tensor([9.0949e-11]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([2.3283e-10]),\n tensor([1.4552e-11]),\n tensor([3.2742e-11]),\n tensor([2.3283e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([1.4552e-11]),\n tensor([0.]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12]),\n tensor([5.2387e-10]),\n tensor([0.]),\n tensor([5.8208e-11]),\n tensor([3.6380e-12]),\n tensor([3.6380e-12])]"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X.shape[1] # <размерность элемента выборки >\n",
    "print(n_inputs)\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = SquaredLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "  for x_example, y_example in zip(X, y):\n",
    "    # нейрон\n",
    "    y_pred =  neuron.forward(x_example)\n",
    "    # функция потерь\n",
    "    curr_loss = loss.forward(y_pred, y_example)\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "    #backward\n",
    "    loss.backward(y_pred, y_example)\n",
    "    neuron.backward(loss.dinput)\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "    neuron.weights -= learning_rate * neuron.dweights\n",
    "    neuron.bias -= learning_rate * neuron.dbias\n",
    "losses[::20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "2.4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "2.4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = 2 * (y_pred - y_true) / y_pred.shape[0] # df/dy^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "2.4.2.2. Модифицируйте класс `Neuron` из __2.4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "o_OpuAP0Jpz1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    self.n_inputs = n_inputs\n",
    "    self.weights = torch.randn(1, n_inputs).T\n",
    "    self.bias = torch.randn(1)\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    return torch.matmul(inputs, self.weights) + self.bias\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
    "    self.dinputs = torch.matmul(dvalue, self.weights.T)\n",
    "    self.dweights = torch.matmul(self.inputs.T, dvalue) # df/dW\n",
    "    self.dbias = torch.sum(dvalue) # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "2.4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "Zqwm_7eqJim1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(14387.2217),\n tensor(6112.7715),\n tensor(2649.1934),\n tensor(1174.4604),\n tensor(534.1511),\n tensor(249.8707),\n tensor(120.4514),\n tensor(59.8823),\n tensor(30.6870),\n tensor(16.1809),\n tensor(8.7543),\n tensor(4.8431),\n tensor(2.7301),\n tensor(1.5627),\n tensor(0.9055),\n tensor(0.5298),\n tensor(0.3124),\n tensor(0.1852),\n tensor(0.1104),\n tensor(0.0660),\n tensor(0.0396),\n tensor(0.0238),\n tensor(0.0143),\n tensor(0.0086),\n tensor(0.0052),\n tensor(0.0031),\n tensor(0.0019),\n tensor(0.0012),\n tensor(0.0007),\n tensor(0.0004),\n tensor(0.0003),\n tensor(0.0002),\n tensor(9.3870e-05),\n tensor(5.7004e-05),\n tensor(3.4591e-05),\n tensor(2.0998e-05),\n tensor(1.2809e-05),\n tensor(7.7580e-06),\n tensor(4.7027e-06),\n tensor(2.8949e-06),\n tensor(1.8024e-06),\n tensor(1.1127e-06),\n tensor(7.0796e-07),\n tensor(4.8205e-07),\n tensor(3.1544e-07),\n tensor(1.8797e-07),\n tensor(1.2597e-07),\n tensor(1.1551e-07),\n tensor(1.1416e-07),\n tensor(1.1121e-07)]"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X.shape[1] # размерность\n",
    "learning_rate = 0.01 #  скорость\n",
    "n_epoch = 1000 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # forward pass\n",
    "    y_pred = neuron.forward(X) # нейрон\n",
    "    curr_loss = loss.forward(y_pred, y) #функция потерь\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "\n",
    "    #backward\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "    loss.backward(y_pred, y)\n",
    "    neuron.backward(loss.dinput)\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "    neuron.weights -= learning_rate * neuron.dweights\n",
    "    neuron.bias -= learning_rate * neuron.dbias\n",
    "\n",
    "losses[::20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "2.4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "2.4.3.1 Модифицируйте класс `Linear` из __2.1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "9zWuhaLdSB2_"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "        # <создать атрибуты объекта weights и biases>\n",
    "        self.n_features = n_features\n",
    "        self.n_neurons = n_neurons\n",
    "        self.weights = torch.randn(n_neurons, n_features).T\n",
    "        self.biases = torch.randn(n_neurons)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return torch.matmul(inputs, self.weights) + self.biases # <реализовать логику слоя>\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = torch.matmul(self.inputs.T, dvalues) # df/dW\n",
    "        self.dbiases = torch.matmul(torch.ones(dvalues.shape[0]), dvalues) # df/db\n",
    "        self.dinputs = torch.matmul(dvalues, self.weights.T) # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "2.4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor(14447.3252),\n tensor(6157.1387),\n tensor(2678.4099),\n tensor(1192.7377),\n tensor(545.2997),\n tensor(256.5868),\n tensor(124.4738),\n tensor(62.2868),\n tensor(32.1240),\n tensor(17.0403),\n tensor(9.2691),\n tensor(5.1520),\n tensor(2.9156),\n tensor(1.6743),\n tensor(0.9728),\n tensor(0.5704),\n tensor(0.3368),\n tensor(0.2000),\n tensor(0.1193),\n tensor(0.0714),\n tensor(0.0428),\n tensor(0.0258),\n tensor(0.0155),\n tensor(0.0094),\n tensor(0.0056),\n tensor(0.0034),\n tensor(0.0021),\n tensor(0.0012),\n tensor(0.0008),\n tensor(0.0005),\n tensor(0.0003),\n tensor(0.0002),\n tensor(0.0001),\n tensor(6.1975e-05),\n tensor(3.7619e-05),\n tensor(2.2861e-05),\n tensor(1.3902e-05),\n tensor(8.4315e-06),\n tensor(5.1295e-06),\n tensor(3.1487e-06),\n tensor(1.9529e-06),\n tensor(1.2146e-06),\n tensor(7.5568e-07),\n tensor(5.1350e-07),\n tensor(3.3873e-07),\n tensor(2.0924e-07),\n tensor(1.3107e-07),\n tensor(1.1485e-07),\n tensor(1.1481e-07),\n tensor(1.1184e-07)]"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X.shape[1]  # <размерность элемента выборки >\n",
    "learning_rate = 0.01  #  скорость обучения\n",
    "n_epoch = 1000  #  количество эпох\n",
    "\n",
    "layer = Linear(n_inputs, 1)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # forward pass\n",
    "    y_pred = layer.forward(X)  # <прогон через нейрон>\n",
    "    curr_loss = loss.forward(y_pred, y)  # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "    loss.backward(y_pred, y)\n",
    "    layer.backward(loss.dinput)\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "    layer.weights -= learning_rate * layer.dweights\n",
    "    layer.biases -= learning_rate * layer.dbiases\n",
    "\n",
    "losses[::20]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "2.4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = inputs.clip(min=0)\n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.clone()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 mean loss 3.6640965938568115\n",
      "epoch 200 mean loss 0.05424844101071358\n",
      "epoch 400 mean loss 0.032951656728982925\n",
      "epoch 600 mean loss 0.020990218967199326\n",
      "epoch 800 mean loss 0.014961766079068184\n",
      "epoch 1000 mean loss 0.012281829491257668\n",
      "epoch 1200 mean loss 0.011160657741129398\n",
      "epoch 1400 mean loss 0.010714847594499588\n",
      "epoch 1600 mean loss 0.010542060248553753\n",
      "epoch 1800 mean loss 0.010476239025592804\n",
      "epoch 2000 mean loss 0.010413460433483124\n"
     ]
    }
   ],
   "source": [
    "# создание компонентов сети\n",
    "fc1 = Linear(X.shape[1], 10)\n",
    "relu1 = Activation_ReLU()\n",
    "fc2 = Linear(10, 1)\n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "    # <forward pass>\n",
    "    y_pred = fc2.forward(relu1.forward(fc1.forward(X)))\n",
    "\n",
    "    data_loss = loss.forward(y_pred, y) # <прогон через функцию потерь>\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'epoch {epoch} mean loss {data_loss}')\n",
    "        ys.append(y_pred)\n",
    "\n",
    "    # <backprop>\n",
    "    # loss > fc2 > relu1 > fc1\n",
    "    loss.backward(y_pred, y)\n",
    "    fc2.backward(loss.dinput)\n",
    "    relu1.backward(fc2.dinputs)\n",
    "    fc1.backward(relu1.dinputs)\n",
    "\n",
    "    # <шаг оптимизации для fc1>\n",
    "    fc1.weights -= lr * fc1.dweights\n",
    "    fc1.biases -= lr * fc1.dbiases\n",
    "\n",
    "    # <шаг оптимизации для fc2>\n",
    "    fc2.weights -= lr * fc2.dweights\n",
    "    fc2.biases -= lr * fc2.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [161]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      3\u001B[0m fig, axs \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;28mlen\u001B[39m(ys), \u001B[38;5;241m1\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m40\u001B[39m))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ax, y_ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(axs, ys):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "    ax.scatter(X.numpy(), y.numpy(), color=\"red\")\n",
    "    ax.plot(X.numpy(), y_.numpy(), 'blue', lw=3)\n",
    "    ax.set_xlim(-1.05, 1.5)\n",
    "    ax.set_ylim(-0.25, 1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
