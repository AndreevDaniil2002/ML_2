{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6ktSXdQVb9Li",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L7Inb1JbiDy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1 Автоматическое дифференцирование в `torch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsZ69HZ0EsI7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.1.1 Воспользовавшись классами `Neuron` и `SquaredLoss` из задачи 2.4.1 и автоматическим дифференцированием, которое предоставляет `torch`, решить задачу регрессии. Для оптимизации использовать стохастический градиетный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        #Атрибут weights\n",
    "        self.W = torch.randn(n_inputs)\n",
    "        #Атрибут bias\n",
    "        self.B = torch.randn(1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return torch.matmul(inputs, self.W.T) + self.B\n",
    "\n",
    "    def backward(self, dvalue):\n",
    "\n",
    "        self.dweights = dvalue * self.inputs\n",
    "        self.dinput =  dvalue * self.W\n",
    "        self.dbias = dvalue\n",
    "\n",
    "        # Возвращаем градиент весов и смещения\n",
    "        return self.dweights, self.dbias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class SquaredLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
    "        y_true = torch.tensor(y_true)\n",
    "        z = (self.y_pred - y_true) ** 2\n",
    "        self.z = z\n",
    "        return z\n",
    "\n",
    "    def backward(self):\n",
    "        self.z.backward()\n",
    "        self.dinput = self.y_pred.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zynPAaOrRKTm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
    "X = torch.from_numpy(X).to(dtype=torch.float32)\n",
    "y = torch.from_numpy(y).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danila\\AppData\\Local\\Temp\\ipykernel_12816\\1712833060.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y_pred = torch.tensor(y_pred, requires_grad=True)\n",
      "C:\\Users\\Danila\\AppData\\Local\\Temp\\ipykernel_12816\\1712833060.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_true = torch.tensor(y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss -> 42.639652252197266\n",
      "Epoch 0 loss -> 812.2446899414062\n",
      "Epoch 0 loss -> 3148.610107421875\n",
      "Epoch 0 loss -> 2100.96435546875\n",
      "Epoch 0 loss -> 485.009521484375\n",
      "Epoch 0 loss -> 93.40496063232422\n",
      "Epoch 0 loss -> 7461.3837890625\n",
      "Epoch 0 loss -> 27.271696090698242\n",
      "Epoch 0 loss -> 1773.739501953125\n",
      "Epoch 0 loss -> 2919.37646484375\n",
      "Epoch 1 loss -> 1483.7337646484375\n",
      "Epoch 1 loss -> 1863.12109375\n",
      "Epoch 1 loss -> 14.973762512207031\n",
      "Epoch 1 loss -> 6003.92041015625\n",
      "Epoch 1 loss -> 124.45703887939453\n",
      "Epoch 1 loss -> 0.6290546655654907\n",
      "Epoch 1 loss -> 11433.376953125\n",
      "Epoch 1 loss -> 4333.65234375\n",
      "Epoch 1 loss -> 954.1820678710938\n",
      "Epoch 1 loss -> 1589.8525390625\n",
      "Epoch 2 loss -> 4326.564453125\n",
      "Epoch 2 loss -> 469.353759765625\n",
      "Epoch 2 loss -> 6271.67529296875\n",
      "Epoch 2 loss -> 1423.5123291015625\n",
      "Epoch 2 loss -> 7396.15771484375\n",
      "Epoch 2 loss -> 885.2360229492188\n",
      "Epoch 2 loss -> 77.0230484008789\n",
      "Epoch 2 loss -> 283.11602783203125\n",
      "Epoch 2 loss -> 953.9406127929688\n",
      "Epoch 2 loss -> 12448.765625\n",
      "Epoch 3 loss -> 1678.1177978515625\n",
      "Epoch 3 loss -> 248.6218719482422\n",
      "Epoch 3 loss -> 182.068359375\n",
      "Epoch 3 loss -> 1420.6495361328125\n",
      "Epoch 3 loss -> 612.5308837890625\n",
      "Epoch 3 loss -> 179.49502563476562\n",
      "Epoch 3 loss -> 3463.08349609375\n",
      "Epoch 3 loss -> 257.8279113769531\n",
      "Epoch 3 loss -> 3740.841064453125\n",
      "Epoch 3 loss -> 934.1409301757812\n",
      "Epoch 4 loss -> 3.132800340652466\n",
      "Epoch 4 loss -> 3561.904541015625\n",
      "Epoch 4 loss -> 1287.7801513671875\n",
      "Epoch 4 loss -> 1389.607177734375\n",
      "Epoch 4 loss -> 256.55926513671875\n",
      "Epoch 4 loss -> 2073.905517578125\n",
      "Epoch 4 loss -> 106.19889068603516\n",
      "Epoch 4 loss -> 178.0056610107422\n",
      "Epoch 4 loss -> 2586.8623046875\n",
      "Epoch 4 loss -> 166.40191650390625\n",
      "Epoch 5 loss -> 1718.7408447265625\n",
      "Epoch 5 loss -> 810.5718994140625\n",
      "Epoch 5 loss -> 246.44151306152344\n",
      "Epoch 5 loss -> 595.8233032226562\n",
      "Epoch 5 loss -> 407.5411682128906\n",
      "Epoch 5 loss -> 420.7170715332031\n",
      "Epoch 5 loss -> 1159.2047119140625\n",
      "Epoch 5 loss -> 470.3891906738281\n",
      "Epoch 5 loss -> 978.0481567382812\n",
      "Epoch 5 loss -> 949.6024780273438\n",
      "Epoch 6 loss -> 226.99484252929688\n",
      "Epoch 6 loss -> 119.8140640258789\n",
      "Epoch 6 loss -> 3080.64453125\n",
      "Epoch 6 loss -> 1331.8072509765625\n",
      "Epoch 6 loss -> 1290.314208984375\n",
      "Epoch 6 loss -> 1121.6072998046875\n",
      "Epoch 6 loss -> 2231.319091796875\n",
      "Epoch 6 loss -> 2698.384521484375\n",
      "Epoch 6 loss -> 14.98027229309082\n",
      "Epoch 6 loss -> 1347.9788818359375\n",
      "Epoch 7 loss -> 35.801124572753906\n",
      "Epoch 7 loss -> 406.4136657714844\n",
      "Epoch 7 loss -> 585.3035278320312\n",
      "Epoch 7 loss -> 366.46282958984375\n",
      "Epoch 7 loss -> 549.24853515625\n",
      "Epoch 7 loss -> 219.8522491455078\n",
      "Epoch 7 loss -> 536.2642211914062\n",
      "Epoch 7 loss -> 152.2975616455078\n",
      "Epoch 7 loss -> 42.204891204833984\n",
      "Epoch 7 loss -> 1.548843502998352\n",
      "Epoch 8 loss -> 1045.7371826171875\n",
      "Epoch 8 loss -> 198.4901123046875\n",
      "Epoch 8 loss -> 118.7814712524414\n",
      "Epoch 8 loss -> 27.44158935546875\n",
      "Epoch 8 loss -> 435.3393859863281\n",
      "Epoch 8 loss -> 355.14910888671875\n",
      "Epoch 8 loss -> 283.4925231933594\n",
      "Epoch 8 loss -> 254.2220916748047\n",
      "Epoch 8 loss -> 242.29014587402344\n",
      "Epoch 8 loss -> 3.1798031330108643\n",
      "Epoch 9 loss -> 282.2369689941406\n",
      "Epoch 9 loss -> 707.9138793945312\n",
      "Epoch 9 loss -> 251.26658630371094\n",
      "Epoch 9 loss -> 8.248579978942871\n",
      "Epoch 9 loss -> 176.465576171875\n",
      "Epoch 9 loss -> 397.868896484375\n",
      "Epoch 9 loss -> 2276.650146484375\n",
      "Epoch 9 loss -> 172.6487579345703\n",
      "Epoch 9 loss -> 101.39353942871094\n",
      "Epoch 9 loss -> 58.610877990722656\n",
      "Epoch 10 loss -> 119.6900405883789\n",
      "Epoch 10 loss -> 1112.95068359375\n",
      "Epoch 10 loss -> 265.4415588378906\n",
      "Epoch 10 loss -> 0.4543309807777405\n",
      "Epoch 10 loss -> 176.6932830810547\n",
      "Epoch 10 loss -> 456.0462646484375\n",
      "Epoch 10 loss -> 260.0198059082031\n",
      "Epoch 10 loss -> 21.056381225585938\n",
      "Epoch 10 loss -> 1107.178955078125\n",
      "Epoch 10 loss -> 474.8460388183594\n",
      "Epoch 11 loss -> 135.39794921875\n",
      "Epoch 11 loss -> 239.6359405517578\n",
      "Epoch 11 loss -> 51.72504806518555\n",
      "Epoch 11 loss -> 16.599321365356445\n",
      "Epoch 11 loss -> 2.9951162338256836\n",
      "Epoch 11 loss -> 61.970123291015625\n",
      "Epoch 11 loss -> 29.270244598388672\n",
      "Epoch 11 loss -> 19.240991592407227\n",
      "Epoch 11 loss -> 182.72181701660156\n",
      "Epoch 11 loss -> 344.8624572753906\n",
      "Epoch 12 loss -> 1.1294819116592407\n",
      "Epoch 12 loss -> 692.565673828125\n",
      "Epoch 12 loss -> 55.42041015625\n",
      "Epoch 12 loss -> 925.1089477539062\n",
      "Epoch 12 loss -> 185.94552612304688\n",
      "Epoch 12 loss -> 9.466638565063477\n",
      "Epoch 12 loss -> 60.68601608276367\n",
      "Epoch 12 loss -> 22.02096176147461\n",
      "Epoch 12 loss -> 176.27610778808594\n",
      "Epoch 12 loss -> 92.92451477050781\n",
      "Epoch 13 loss -> 1.8898802995681763\n",
      "Epoch 13 loss -> 82.15725708007812\n",
      "Epoch 13 loss -> 100.81295776367188\n",
      "Epoch 13 loss -> 146.0883331298828\n",
      "Epoch 13 loss -> 52.32900619506836\n",
      "Epoch 13 loss -> 0.03591417148709297\n",
      "Epoch 13 loss -> 86.68635559082031\n",
      "Epoch 13 loss -> 3.0933825969696045\n",
      "Epoch 13 loss -> 67.29876708984375\n",
      "Epoch 13 loss -> 37.07969284057617\n",
      "Epoch 14 loss -> 95.53827667236328\n",
      "Epoch 14 loss -> 411.3575744628906\n",
      "Epoch 14 loss -> 27.089780807495117\n",
      "Epoch 14 loss -> 27.695158004760742\n",
      "Epoch 14 loss -> 80.86876678466797\n",
      "Epoch 14 loss -> 38.07024383544922\n",
      "Epoch 14 loss -> 15.905048370361328\n",
      "Epoch 14 loss -> 8.098276138305664\n",
      "Epoch 14 loss -> 4.576659202575684\n",
      "Epoch 14 loss -> 133.44239807128906\n",
      "Epoch 15 loss -> 55.38554000854492\n",
      "Epoch 15 loss -> 12.380341529846191\n",
      "Epoch 15 loss -> 27.27235221862793\n",
      "Epoch 15 loss -> 27.790679931640625\n",
      "Epoch 15 loss -> 3.815390110015869\n",
      "Epoch 15 loss -> 45.78297424316406\n",
      "Epoch 15 loss -> 30.849998474121094\n",
      "Epoch 15 loss -> 45.70768737792969\n",
      "Epoch 15 loss -> 3.9011313915252686\n",
      "Epoch 15 loss -> 48.193607330322266\n",
      "Epoch 16 loss -> 0.5680525898933411\n",
      "Epoch 16 loss -> 48.705963134765625\n",
      "Epoch 16 loss -> 216.63711547851562\n",
      "Epoch 16 loss -> 12.659299850463867\n",
      "Epoch 16 loss -> 0.10724960267543793\n",
      "Epoch 16 loss -> 32.60565948486328\n",
      "Epoch 16 loss -> 55.31618118286133\n",
      "Epoch 16 loss -> 1.2905689477920532\n",
      "Epoch 16 loss -> 4.1559247970581055\n",
      "Epoch 16 loss -> 3.21651554107666\n",
      "Epoch 17 loss -> 9.054619789123535\n",
      "Epoch 17 loss -> 6.78896427154541\n",
      "Epoch 17 loss -> 0.8867130875587463\n",
      "Epoch 17 loss -> 26.12154197692871\n",
      "Epoch 17 loss -> 6.215900421142578\n",
      "Epoch 17 loss -> 0.1197705864906311\n",
      "Epoch 17 loss -> 35.65749740600586\n",
      "Epoch 17 loss -> 34.352394104003906\n",
      "Epoch 17 loss -> 2.978752851486206\n",
      "Epoch 17 loss -> 6.075924873352051\n",
      "Epoch 18 loss -> 26.318828582763672\n",
      "Epoch 18 loss -> 36.275596618652344\n",
      "Epoch 18 loss -> 17.848323822021484\n",
      "Epoch 18 loss -> 63.32988739013672\n",
      "Epoch 18 loss -> 0.4105861485004425\n",
      "Epoch 18 loss -> 0.2407563179731369\n",
      "Epoch 18 loss -> 98.38790130615234\n",
      "Epoch 18 loss -> 17.864700317382812\n",
      "Epoch 18 loss -> 16.156370162963867\n",
      "Epoch 18 loss -> 22.016450881958008\n",
      "Epoch 19 loss -> 3.2779695987701416\n",
      "Epoch 19 loss -> 2.961285352706909\n",
      "Epoch 19 loss -> 7.142004489898682\n",
      "Epoch 19 loss -> 15.088351249694824\n",
      "Epoch 19 loss -> 0.8415324091911316\n",
      "Epoch 19 loss -> 13.751104354858398\n",
      "Epoch 19 loss -> 10.065510749816895\n",
      "Epoch 19 loss -> 3.6524460315704346\n",
      "Epoch 19 loss -> 3.1806271076202393\n",
      "Epoch 19 loss -> 10.657156944274902\n",
      "Epoch 20 loss -> 8.632761001586914\n",
      "Epoch 20 loss -> 91.98471069335938\n",
      "Epoch 20 loss -> 35.13749313354492\n",
      "Epoch 20 loss -> 24.673542022705078\n",
      "Epoch 20 loss -> 6.711104869842529\n",
      "Epoch 20 loss -> 0.12600572407245636\n",
      "Epoch 20 loss -> 3.066894292831421\n",
      "Epoch 20 loss -> 10.267391204833984\n",
      "Epoch 20 loss -> 2.2807400226593018\n",
      "Epoch 20 loss -> 14.270674705505371\n",
      "Epoch 21 loss -> 5.056339263916016\n",
      "Epoch 21 loss -> 3.3295905590057373\n",
      "Epoch 21 loss -> 4.159269332885742\n",
      "Epoch 21 loss -> 6.119198799133301\n",
      "Epoch 21 loss -> 18.310945510864258\n",
      "Epoch 21 loss -> 29.266653060913086\n",
      "Epoch 21 loss -> 0.17449703812599182\n",
      "Epoch 21 loss -> 1.940372347831726\n",
      "Epoch 21 loss -> 20.054983139038086\n",
      "Epoch 21 loss -> 0.6621351838111877\n",
      "Epoch 22 loss -> 0.6393458843231201\n",
      "Epoch 22 loss -> 28.871191024780273\n",
      "Epoch 22 loss -> 24.332744598388672\n",
      "Epoch 22 loss -> 1.0530459880828857\n",
      "Epoch 22 loss -> 5.435803413391113\n",
      "Epoch 22 loss -> 0.819036602973938\n",
      "Epoch 22 loss -> 0.04221051186323166\n",
      "Epoch 22 loss -> 10.601017951965332\n",
      "Epoch 22 loss -> 1.2058459520339966\n",
      "Epoch 22 loss -> 14.749849319458008\n",
      "Epoch 23 loss -> 6.715117454528809\n",
      "Epoch 23 loss -> 4.561916351318359\n",
      "Epoch 23 loss -> 11.17265510559082\n",
      "Epoch 23 loss -> 0.2641175091266632\n",
      "Epoch 23 loss -> 0.10933342576026917\n",
      "Epoch 23 loss -> 3.6644129753112793\n",
      "Epoch 23 loss -> 1.3441840410232544\n",
      "Epoch 23 loss -> 3.7871205806732178\n",
      "Epoch 23 loss -> 4.442674160003662\n",
      "Epoch 23 loss -> 5.214845657348633\n",
      "Epoch 24 loss -> 1.1216628551483154\n",
      "Epoch 24 loss -> 6.178906440734863\n",
      "Epoch 24 loss -> 2.5760326385498047\n",
      "Epoch 24 loss -> 0.34752368927001953\n",
      "Epoch 24 loss -> 8.1315279006958\n",
      "Epoch 24 loss -> 2.1302566528320312\n",
      "Epoch 24 loss -> 3.692049503326416\n",
      "Epoch 24 loss -> 2.2612314224243164\n",
      "Epoch 24 loss -> 0.9871895909309387\n",
      "Epoch 24 loss -> 5.047987937927246\n",
      "Epoch 25 loss -> 4.194789886474609\n",
      "Epoch 25 loss -> 0.5156423449516296\n",
      "Epoch 25 loss -> 7.126079559326172\n",
      "Epoch 25 loss -> 2.0639941692352295\n",
      "Epoch 25 loss -> 1.038522481918335\n",
      "Epoch 25 loss -> 0.21232976019382477\n",
      "Epoch 25 loss -> 1.4147545099258423\n",
      "Epoch 25 loss -> 1.6856169700622559\n",
      "Epoch 25 loss -> 0.7548750638961792\n",
      "Epoch 25 loss -> 0.8525635004043579\n",
      "Epoch 26 loss -> 0.06494191288948059\n",
      "Epoch 26 loss -> 0.7685651183128357\n",
      "Epoch 26 loss -> 17.5860538482666\n",
      "Epoch 26 loss -> 0.7877862453460693\n",
      "Epoch 26 loss -> 0.025441201403737068\n",
      "Epoch 26 loss -> 0.043806109577417374\n",
      "Epoch 26 loss -> 2.8666133880615234\n",
      "Epoch 26 loss -> 1.5867475271224976\n",
      "Epoch 26 loss -> 1.8408236503601074\n",
      "Epoch 26 loss -> 0.2702021598815918\n",
      "Epoch 27 loss -> 0.07538475096225739\n",
      "Epoch 27 loss -> 8.656673431396484\n",
      "Epoch 27 loss -> 1.0032756328582764\n",
      "Epoch 27 loss -> 5.273497104644775\n",
      "Epoch 27 loss -> 1.208251714706421\n",
      "Epoch 27 loss -> 0.004246420692652464\n",
      "Epoch 27 loss -> 0.7917255163192749\n",
      "Epoch 27 loss -> 1.8271130323410034\n",
      "Epoch 27 loss -> 0.6320766806602478\n",
      "Epoch 27 loss -> 4.029044151306152\n",
      "Epoch 28 loss -> 0.21923688054084778\n",
      "Epoch 28 loss -> 0.0029022041708230972\n",
      "Epoch 28 loss -> 3.8896872997283936\n",
      "Epoch 28 loss -> 0.0011166790500283241\n",
      "Epoch 28 loss -> 0.2015610784292221\n",
      "Epoch 28 loss -> 0.0002083638682961464\n",
      "Epoch 28 loss -> 0.7940285205841064\n",
      "Epoch 28 loss -> 0.009221256710588932\n",
      "Epoch 28 loss -> 0.7231525182723999\n",
      "Epoch 28 loss -> 0.6285635232925415\n",
      "Epoch 29 loss -> 1.2196755409240723\n",
      "Epoch 29 loss -> 0.007191809359937906\n",
      "Epoch 29 loss -> 0.41004613041877747\n",
      "Epoch 29 loss -> 0.00814493652433157\n",
      "Epoch 29 loss -> 0.7175126075744629\n",
      "Epoch 29 loss -> 2.510239362716675\n",
      "Epoch 29 loss -> 0.17714586853981018\n",
      "Epoch 29 loss -> 0.9057695865631104\n",
      "Epoch 29 loss -> 2.491417646408081\n",
      "Epoch 29 loss -> 0.47712254524230957\n",
      "Epoch 30 loss -> 0.4731520414352417\n",
      "Epoch 30 loss -> 0.4641244411468506\n",
      "Epoch 30 loss -> 0.23450829088687897\n",
      "Epoch 30 loss -> 0.3405834436416626\n",
      "Epoch 30 loss -> 1.0320831537246704\n",
      "Epoch 30 loss -> 2.8331522941589355\n",
      "Epoch 30 loss -> 0.0012307371944189072\n",
      "Epoch 30 loss -> 0.09349990636110306\n",
      "Epoch 30 loss -> 3.3384156227111816\n",
      "Epoch 30 loss -> 0.24901677668094635\n",
      "Epoch 31 loss -> 4.666143417358398\n",
      "Epoch 31 loss -> 3.2149012088775635\n",
      "Epoch 31 loss -> 0.6954472064971924\n",
      "Epoch 31 loss -> 0.5727198123931885\n",
      "Epoch 31 loss -> 0.06534110009670258\n",
      "Epoch 31 loss -> 0.1202344223856926\n",
      "Epoch 31 loss -> 0.3181682229042053\n",
      "Epoch 31 loss -> 0.11651759594678879\n",
      "Epoch 31 loss -> 0.059742558747529984\n",
      "Epoch 31 loss -> 0.07005570828914642\n",
      "Epoch 32 loss -> 0.026380345225334167\n",
      "Epoch 32 loss -> 0.16245195269584656\n",
      "Epoch 32 loss -> 0.6162546873092651\n",
      "Epoch 32 loss -> 0.033407580107450485\n",
      "Epoch 32 loss -> 1.1669808626174927\n",
      "Epoch 32 loss -> 0.0422983355820179\n",
      "Epoch 32 loss -> 0.23805922269821167\n",
      "Epoch 32 loss -> 0.08569104969501495\n",
      "Epoch 32 loss -> 0.08937349915504456\n",
      "Epoch 32 loss -> 0.012765317223966122\n",
      "Epoch 33 loss -> 0.12407677620649338\n",
      "Epoch 33 loss -> 0.06532549858093262\n",
      "Epoch 33 loss -> 0.1452805995941162\n",
      "Epoch 33 loss -> 0.0020549860782921314\n",
      "Epoch 33 loss -> 0.15977242588996887\n",
      "Epoch 33 loss -> 0.146539568901062\n",
      "Epoch 33 loss -> 0.011679206043481827\n",
      "Epoch 33 loss -> 0.4252040386199951\n",
      "Epoch 33 loss -> 0.2513158917427063\n",
      "Epoch 33 loss -> 1.2244913578033447\n",
      "Epoch 34 loss -> 0.061477113515138626\n",
      "Epoch 34 loss -> 0.07979939877986908\n",
      "Epoch 34 loss -> 0.26755157113075256\n",
      "Epoch 34 loss -> 0.059504568576812744\n",
      "Epoch 34 loss -> 0.19984525442123413\n",
      "Epoch 34 loss -> 0.24127881228923798\n",
      "Epoch 34 loss -> 0.036548785865306854\n",
      "Epoch 34 loss -> 0.021231401711702347\n",
      "Epoch 34 loss -> 0.3083091080188751\n",
      "Epoch 34 loss -> 0.08603868633508682\n",
      "Epoch 35 loss -> 0.016478685662150383\n",
      "Epoch 35 loss -> 0.032020825892686844\n",
      "Epoch 35 loss -> 0.5933883786201477\n",
      "Epoch 35 loss -> 0.006508783437311649\n",
      "Epoch 35 loss -> 0.08948986232280731\n",
      "Epoch 35 loss -> 0.061949990689754486\n",
      "Epoch 35 loss -> 0.12067131698131561\n",
      "Epoch 35 loss -> 0.5122785568237305\n",
      "Epoch 35 loss -> 0.17204217612743378\n",
      "Epoch 35 loss -> 0.38365638256073\n",
      "Epoch 36 loss -> 0.17607496678829193\n",
      "Epoch 36 loss -> 0.2014686018228531\n",
      "Epoch 36 loss -> 0.07821998000144958\n",
      "Epoch 36 loss -> 0.08253258466720581\n",
      "Epoch 36 loss -> 0.014471243135631084\n",
      "Epoch 36 loss -> 0.003973773214966059\n",
      "Epoch 36 loss -> 0.09039364755153656\n",
      "Epoch 36 loss -> 0.1449579894542694\n",
      "Epoch 36 loss -> 0.6044886708259583\n",
      "Epoch 36 loss -> 0.007093238178640604\n",
      "Epoch 37 loss -> 0.08197243511676788\n",
      "Epoch 37 loss -> 0.04543781280517578\n",
      "Epoch 37 loss -> 0.037228140980005264\n",
      "Epoch 37 loss -> 0.04729410260915756\n",
      "Epoch 37 loss -> 0.061982277780771255\n",
      "Epoch 37 loss -> 0.4904016852378845\n",
      "Epoch 37 loss -> 0.04253717511892319\n",
      "Epoch 37 loss -> 0.03768810257315636\n",
      "Epoch 37 loss -> 0.05445880442857742\n",
      "Epoch 37 loss -> 0.26053035259246826\n",
      "Epoch 38 loss -> 0.13140016794204712\n",
      "Epoch 38 loss -> 0.0032103215344250202\n",
      "Epoch 38 loss -> 0.06669958680868149\n",
      "Epoch 38 loss -> 0.21901188790798187\n",
      "Epoch 38 loss -> 0.11141819506883621\n",
      "Epoch 38 loss -> 0.30693385004997253\n",
      "Epoch 38 loss -> 0.14252537488937378\n",
      "Epoch 38 loss -> 0.2088426947593689\n",
      "Epoch 38 loss -> 0.0010076817125082016\n",
      "Epoch 38 loss -> 0.005060940980911255\n",
      "Epoch 39 loss -> 0.020598366856575012\n",
      "Epoch 39 loss -> 0.021923977881669998\n",
      "Epoch 39 loss -> 0.1893158107995987\n",
      "Epoch 39 loss -> 0.01240584533661604\n",
      "Epoch 39 loss -> 0.10831286013126373\n",
      "Epoch 39 loss -> 0.040149983018636703\n",
      "Epoch 39 loss -> 0.06773606687784195\n",
      "Epoch 39 loss -> 0.024104954674839973\n",
      "Epoch 39 loss -> 0.003055321518331766\n",
      "Epoch 39 loss -> 0.30583587288856506\n",
      "Epoch 40 loss -> 0.006535123568028212\n",
      "Epoch 40 loss -> 0.13255038857460022\n",
      "Epoch 40 loss -> 0.08475671708583832\n",
      "Epoch 40 loss -> 0.0417010597884655\n",
      "Epoch 40 loss -> 0.06067483872175217\n",
      "Epoch 40 loss -> 0.0007992336759343743\n",
      "Epoch 40 loss -> 0.10143528878688812\n",
      "Epoch 40 loss -> 0.025370974093675613\n",
      "Epoch 40 loss -> 0.01838764362037182\n",
      "Epoch 40 loss -> 0.0450645349919796\n",
      "Epoch 41 loss -> 5.423251786851324e-05\n",
      "Epoch 41 loss -> 0.01911790668964386\n",
      "Epoch 41 loss -> 0.0032911193557083607\n",
      "Epoch 41 loss -> 0.003560334676876664\n",
      "Epoch 41 loss -> 0.00014696974540129304\n",
      "Epoch 41 loss -> 0.013224508613348007\n",
      "Epoch 41 loss -> 0.10252191126346588\n",
      "Epoch 41 loss -> 0.041903845965862274\n",
      "Epoch 41 loss -> 0.017811838537454605\n",
      "Epoch 41 loss -> 0.02269083261489868\n",
      "Epoch 42 loss -> 0.008546147495508194\n",
      "Epoch 42 loss -> 0.005648026242852211\n",
      "Epoch 42 loss -> 0.0008623349131084979\n",
      "Epoch 42 loss -> 0.0026921159587800503\n",
      "Epoch 42 loss -> 0.06438317894935608\n",
      "Epoch 42 loss -> 0.009186855517327785\n",
      "Epoch 42 loss -> 0.03821554780006409\n",
      "Epoch 42 loss -> 0.007943548262119293\n",
      "Epoch 42 loss -> 0.07645475119352341\n",
      "Epoch 42 loss -> 0.023014921694993973\n",
      "Epoch 43 loss -> 0.09741798043251038\n",
      "Epoch 43 loss -> 0.0075287120416760445\n",
      "Epoch 43 loss -> 0.005443497095257044\n",
      "Epoch 43 loss -> 0.010290160775184631\n",
      "Epoch 43 loss -> 0.011625673621892929\n",
      "Epoch 43 loss -> 0.06503527611494064\n",
      "Epoch 43 loss -> 0.003630900988355279\n",
      "Epoch 43 loss -> 0.0064627015963196754\n",
      "Epoch 43 loss -> 0.006562443450093269\n",
      "Epoch 43 loss -> 0.04248369112610817\n",
      "Epoch 44 loss -> 0.027136625722050667\n",
      "Epoch 44 loss -> 0.010063069872558117\n",
      "Epoch 44 loss -> 0.01896715722978115\n",
      "Epoch 44 loss -> 0.004941701889038086\n",
      "Epoch 44 loss -> 0.0002444760175421834\n",
      "Epoch 44 loss -> 0.005563485436141491\n",
      "Epoch 44 loss -> 0.0002574297250248492\n",
      "Epoch 44 loss -> 0.0005074110813438892\n",
      "Epoch 44 loss -> 0.0021413001231849194\n",
      "Epoch 44 loss -> 0.004651125054806471\n",
      "Epoch 45 loss -> 0.00046741560800001025\n",
      "Epoch 45 loss -> 0.002642469247803092\n",
      "Epoch 45 loss -> 0.008515845984220505\n",
      "Epoch 45 loss -> 0.010362261906266212\n",
      "Epoch 45 loss -> 0.002727263141423464\n",
      "Epoch 45 loss -> 0.0004741197917610407\n",
      "Epoch 45 loss -> 0.00015550701937172562\n",
      "Epoch 45 loss -> 0.000439715338870883\n",
      "Epoch 45 loss -> 0.016193704679608345\n",
      "Epoch 45 loss -> 0.027743879705667496\n",
      "Epoch 46 loss -> 1.5784786228323355e-05\n",
      "Epoch 46 loss -> 0.008584982715547085\n",
      "Epoch 46 loss -> 0.0006231702864170074\n",
      "Epoch 46 loss -> 0.002844337373971939\n",
      "Epoch 46 loss -> 0.004232263192534447\n",
      "Epoch 46 loss -> 0.0034834779798984528\n",
      "Epoch 46 loss -> 0.03163563087582588\n",
      "Epoch 46 loss -> 0.0006435220711864531\n",
      "Epoch 46 loss -> 0.0005812384188175201\n",
      "Epoch 46 loss -> 0.02163461409509182\n",
      "Epoch 47 loss -> 0.0013404759811237454\n",
      "Epoch 47 loss -> 0.0005218844744376838\n",
      "Epoch 47 loss -> 0.00011572265066206455\n",
      "Epoch 47 loss -> 0.003303057048469782\n",
      "Epoch 47 loss -> 0.0030241943895816803\n",
      "Epoch 47 loss -> 0.004264586605131626\n",
      "Epoch 47 loss -> 0.003306127153337002\n",
      "Epoch 47 loss -> 0.017893388867378235\n",
      "Epoch 47 loss -> 0.0027312489692121744\n",
      "Epoch 47 loss -> 0.00682459119707346\n",
      "Epoch 48 loss -> 0.006126186344772577\n",
      "Epoch 48 loss -> 0.0008649133378639817\n",
      "Epoch 48 loss -> 6.380776176229119e-05\n",
      "Epoch 48 loss -> 9.922413482854608e-06\n",
      "Epoch 48 loss -> 8.272996637970209e-06\n",
      "Epoch 48 loss -> 0.002403269289061427\n",
      "Epoch 48 loss -> 0.0016651146579533815\n",
      "Epoch 48 loss -> 0.0054508172906935215\n",
      "Epoch 48 loss -> 0.0004276406252756715\n",
      "Epoch 48 loss -> 0.0020579269621521235\n",
      "Epoch 49 loss -> 0.003918180242180824\n",
      "Epoch 49 loss -> 0.0005271044792607427\n",
      "Epoch 49 loss -> 0.0013583424733951688\n",
      "Epoch 49 loss -> 0.0015378231182694435\n",
      "Epoch 49 loss -> 0.0004733310197480023\n",
      "Epoch 49 loss -> 0.00023960215912666172\n",
      "Epoch 49 loss -> 0.0010166018037125468\n",
      "Epoch 49 loss -> 0.0026079474482685328\n",
      "Epoch 49 loss -> 0.009843149222433567\n",
      "Epoch 49 loss -> 0.0018118898151442409\n"
     ]
    }
   ],
   "source": [
    "# <размерность элемента выборки >\n",
    "n_inputs = 4\n",
    "# скорость обучения\n",
    "learning_rate = 0.01\n",
    "# количество эпох\n",
    "n_epoch = 50\n",
    "#размер пакета\n",
    "batch_size = 10\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = SquaredLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epoch):\n",
    "    sample = torch.randint(0, X.shape[0], size=(batch_size,))\n",
    "    for x_example, y_example in zip(X[sample], y[sample]):\n",
    "        # Активация\n",
    "        y_pred = neuron.forward(x_example)\n",
    "        curr_loss = loss.forward(y_pred, y_example)\n",
    "        losses.append(curr_loss)\n",
    "\n",
    "        # Обратное распространение\n",
    "        loss.backward()\n",
    "        dweights, dbias = neuron.backward(loss.dinput)\n",
    "\n",
    "        # Обновление вестов\n",
    "        neuron.W -= learning_rate * dweights\n",
    "        neuron.B -= learning_rate * dbias\n",
    "        print(f\"Epoch {epoch} loss -> {curr_loss[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxWeyJw5lAqU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " 3.1.2 Воспользовавшись классами `Linear` и `MSELoss` из задачи 2.1.4 и 2.3.1, `ReLU` из 2.2.1 и автоматическим дифференцированием, которое предоставляет `torch`, решить задачу регрессии. Для оптимизации использовать пакетный градиентный спуск. Вывести график функции потерь в зависимости от номера эпохи. Вывести на одном графике исходные данные и предсказанные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnlAt1NEQoat",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nifm0FVB2y5N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Алгоритмы оптимизации в `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "u5PTTYou3xx8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oBFfJpmcwfn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.2.1 Решить задачу 3.1.1, воспользовавшись оптимизатором `optim.SDG` для применения стохастического градиентого спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LFAacdy46bX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.2.2 Решить задачу 3.1.2, воспользовавшись оптимизатором `optim.Adam` для применения пакетного градиентого спуска. Вывести график функции потерь в зависимости от номера эпохи. Вывести на одном графике исходные данные и предсказанные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-qUqdALiN-G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 Построение сетей при помощи `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vxsck-1M6TAV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0ICJtarif3_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.3.1 Решить задачу регрессии, соблюдая следующие условия:\n",
    "\n",
    "1. Оформить нейронную сеть в виде класса - наследника `nn.Module`\n",
    "2. При создании сети использовать готовые блоки из `torch.nn`: слои, функции активации, функции потерь и т.д.\n",
    "3. Для оптимизации использовать любой алгоритм оптимизации из `torch.optim` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "L1bvXHhO7aWs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.554947018623352\n",
      "Epoch 100 loss: 0.3385131359100342\n",
      "Epoch 200 loss: 0.18949802219867706\n",
      "Epoch 300 loss: 0.18628758192062378\n",
      "Epoch 400 loss: 0.18250218033790588\n",
      "Epoch 500 loss: 0.17690861225128174\n",
      "Epoch 600 loss: 0.1686611771583557\n",
      "Epoch 700 loss: 0.15727734565734863\n",
      "Epoch 800 loss: 0.14443351328372955\n",
      "Epoch 900 loss: 0.1323504000902176\n",
      "Epoch 1000 loss: 0.12031371891498566\n",
      "Epoch 1100 loss: 0.10165800899267197\n",
      "Epoch 1200 loss: 0.07109122723340988\n",
      "Epoch 1300 loss: 0.043266259133815765\n",
      "Epoch 1400 loss: 0.025755923241376877\n",
      "Epoch 1500 loss: 0.017111917957663536\n",
      "Epoch 1600 loss: 0.0133848050609231\n",
      "Epoch 1700 loss: 0.01180289313197136\n",
      "Epoch 1800 loss: 0.010959554463624954\n",
      "Epoch 1900 loss: 0.010262860916554928\n"
     ]
    }
   ],
   "source": [
    "class SineNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(SineNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n",
    "        self.act1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "neuron = SineNet(3)\n",
    "optimizer = optim.Adam(neuron.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = neuron.forward(X)\n",
    "    loss_val = loss(y_pred, y)\n",
    "\n",
    "    loss_val.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0: print(f\"Epoch {epoch} loss: {loss_val}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPUW6fm5jbQd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.3.2 Решить задачу регрессии, соблюдая следующие условия:\n",
    "\n",
    "1. Оформить нейронную сеть в виде объекта `nn.Sequential`\n",
    "2. При создании сети использовать готовые блоки из `torch.nn`: слои, функции активации, функции потерь и т.д.\n",
    "3. Для оптимизации использовать любой алгоритм оптимизации из `torch.optim` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBwbAEd57a2r",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "y = torch.sin(2 * np.pi * X) + 0.1 * torch.rand(X.size())\n",
    "\n",
    "layers = [\n",
    "    torch.nn.Linear(1, 5),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(5, 1)\n",
    "]\n",
    "model = torch.nn.Sequential(*layers)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model.forward(X)\n",
    "    loss_val = loss(y_pred, y)\n",
    "\n",
    "    loss_val.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0: print(f\"Epoch {epoch} loss: {loss_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQj0oVeLj2A1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.4. Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "c82tAkXMjajm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoFPckkp8yhz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    3.4.1 Создать датасет, поставляющий данные из задачи 3.1.2.\n",
    "\n",
    "    Создать `DataLoader` на основе этого датасета и проверить работоспособность.\n",
    "\n",
    "    Воспользовавшись результатами 3.3.1 (или 3.3.2) обучите модель, пользуясь мини-пакетным градиентным спуском с размером пакета (`batch_size`) = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tlcwQzCFRvFc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SinDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "        self.y = torch.sin(2 * np.pi * self.X) + 0.1 * torch.rand(self.X.size())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "sin = SinDataset()\n",
    "X, y = sin.__getitem__(idx=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.6816445589065552\n",
      "Epoch 100 loss: 6.212790594872786e-06\n",
      "Epoch 200 loss: 7.29048821312972e-10\n",
      "Epoch 300 loss: 1.4210854715202004e-14\n",
      "Epoch 400 loss: 0.0\n",
      "Epoch 500 loss: 0.0\n",
      "Epoch 600 loss: 0.0\n",
      "Epoch 700 loss: 0.0\n",
      "Epoch 800 loss: 0.0\n",
      "Epoch 900 loss: 0.0\n",
      "Epoch 1000 loss: 0.0\n",
      "Epoch 1100 loss: 0.0\n",
      "Epoch 1200 loss: 0.0\n",
      "Epoch 1300 loss: 0.0\n",
      "Epoch 1400 loss: 0.0\n",
      "Epoch 1500 loss: 0.0\n",
      "Epoch 1600 loss: 0.0\n",
      "Epoch 1700 loss: 0.0\n",
      "Epoch 1800 loss: 0.0\n",
      "Epoch 1900 loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "class SineNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(SineNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1, n_hidden_neurons)\n",
    "        self.act1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "neuron = SineNet(3)\n",
    "optimizer = optim.Adam(neuron.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = neuron.forward(X)\n",
    "    loss_val = loss(y_pred, y)\n",
    "\n",
    "    loss_val.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0: print(f\"Epoch {epoch} loss: {loss_val}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxz02a3k_VQL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.4.2 Предсказание цен алмазов\n",
    "\n",
    "3.4.2.1 Создайте датасет на основе файла diamonds.csv. \n",
    "\n",
    "1. Удалите все нечисловые столбцы\n",
    "2. Целевой столбец (`y`) - `price`\n",
    "3. Преобразуйте данные в тензоры корректных размеров\n",
    "\n",
    "3.4.2.2 Разбейте датасет на обучающий и тестовый датасет при помощи `torch.utils.data.random_split`.\n",
    "\n",
    "3.4.2.3 Обучите модель для предсказания цен при помощи мини-пакетного градиентного спуска (`batch_size = 256`). \n",
    "\n",
    "3.4.2.4 Выведите график функции потерь в зависимости от номера эпохи (значение потерь для эпохи рассчитывайте как среднее значение ошибок на каждом батче). Проверьте качество модели на тестовой выборке. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEfTNJQI8emD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DiamondsDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "    pass\n",
    "\n",
    "  def __len__(self):\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE81cgQdGM7I",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.4.3 Модифицируйте метод `__init__` датасета из 3.4.2 таким образом, чтобы он мог принимать параметр `transform: callable`. Реализуйте класс `DropColsTransform` для удаления нечисловых данных из массива. Реализуйте класс `ToTensorTransorm` для трансформации массива в тензор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J02LNj_F8qxK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DiamondsDataset(Dataset):\n",
    "  def __init__(self, data, transform):\n",
    "    # ....\n",
    "    self.transform = transform\n",
    "    # ....\n",
    "\n",
    "  def __len__(self):\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # ...\n",
    "    sample = self.X[idx], self.y[idx]\n",
    "    if self.transform:\n",
    "      sample = self.transform(sample)\n",
    "    # ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJii-22pHIlU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DropColsTransform:\n",
    "  def __init__(self, drop):\n",
    "    pass\n",
    "  \n",
    "  def __call__(self, sample):\n",
    "    X, y = sample\n",
    "    # <удаление из X столбцов self.drop>\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZZ-OKrVHnY5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ToTensorTransform:\n",
    "  def __call__(self, sample):\n",
    "    X, y = sample\n",
    "    # <преобразование X и y в тензоры>\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GssBjT9JHt5g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "drop = DropColsTransform(drop=[1, 2, 3])\n",
    "to_tensor = ToTensorTransform()\n",
    "dataset = DiamondsDataset(data, transforms.compose([drop, to_tensor]))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "blank_03_autograd_optim_nn_datasets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}